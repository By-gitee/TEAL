"""
ICSR SVE Baseline vs SVE2 Compact æ€§èƒ½å¯¹æ¯”è„šæœ¬

è¯¥è„šæœ¬ä¸“é—¨ç”¨äºå¯¹æ¯”ä¸¤ä¸ª ICSR ç¨€ç–åŒ–ç®—å­çš„æ€§èƒ½ï¼š
1. thr_sparsify_to_icsr_sve_baseline - SVE ç‰ˆæœ¬ï¼ˆä¸ä½¿ç”¨ svcompactï¼‰
2. thr_sparsify_to_icsr_sve - SVE2 ç‰ˆæœ¬ï¼ˆä½¿ç”¨ svcompact_u32 æŒ‡ä»¤ï¼‰

æµ‹è¯•é‡ç‚¹ï¼š
- é‡åŒ– SVE2 compact æŒ‡ä»¤å¸¦æ¥çš„å®é™…æ€§èƒ½æå‡
- åœ¨ä¸åŒçŸ©é˜µå°ºå¯¸å’Œç¨€ç–åº¦ä¸‹çš„è¡¨ç°
- æä¾›è¯¦ç»†çš„æ€§èƒ½åˆ†æå’Œå¯è§†åŒ–ç»“æœ
"""

import torch
import numpy as np
import time
import argparse
from pathlib import Path
import sys
from typing import Tuple, Dict, List

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
ROOT = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(ROOT))

from kernels.sve_sparse_gemm import (
    thr_sparsify_to_icsr_sve,
    thr_sparsify_to_icsr_sve_baseline,
    load_sve_sparse_gemm_extension,
)


def generate_sparse_tensor(M: int, K: int, sparsity: float, seed: int = 42) -> torch.Tensor:
    """ç”ŸæˆæŒ‡å®šç¨€ç–åº¦çš„å¼ é‡"""
    torch.manual_seed(seed)
    activation = torch.randn(M, K, dtype=torch.float32)
    mask = torch.rand(M, K) > sparsity
    return activation * mask.float()


def warmup_runs(func, activation: torch.Tensor, threshold: float, n: int = 5):
    """é¢„çƒ­è¿è¡Œ"""
    for _ in range(n):
        func(activation, threshold)


def measure_performance(
    func,
    activation: torch.Tensor,
    threshold: float,
    repeats: int = 50
) -> Tuple[float, float]:
    """
    æµ‹é‡å‡½æ•°æ€§èƒ½
    è¿”å›ï¼š(å¹³å‡å»¶è¿Ÿ(ç§’), æ ‡å‡†å·®(ç§’))
    """
    times = []
    for _ in range(repeats):
        start = time.perf_counter()
        func(activation, threshold)
        end = time.perf_counter()
        times.append(end - start)
    
    times = np.array(times)
    return np.mean(times), np.std(times)


def verify_output_consistency(
    activation: torch.Tensor,
    threshold: float
) -> bool:
    """éªŒè¯ä¸¤ä¸ªç®—å­è¾“å‡ºçš„ä¸€è‡´æ€§"""
    nz_counts_sve2, col_indices_sve2, row_offsets_sve2 = thr_sparsify_to_icsr_sve(
        activation, threshold
    )
    nz_counts_baseline, col_indices_baseline, row_offsets_baseline = thr_sparsify_to_icsr_sve_baseline(
        activation, threshold
    )
    
    checks = [
        torch.equal(nz_counts_sve2, nz_counts_baseline),
        torch.equal(col_indices_sve2, col_indices_baseline),
        torch.equal(row_offsets_sve2, row_offsets_baseline),
    ]
    
    return all(checks)


def run_single_benchmark(
    M: int,
    K: int,
    threshold: float,
    sparsity: float,
    warmup: int = 5,
    repeats: int = 50,
    verbose: bool = True
) -> Dict:
    """è¿è¡Œå•ä¸ªé…ç½®çš„æ€§èƒ½æµ‹è¯•"""
    
    if verbose:
        print(f"\n{'='*80}")
        print(f"æµ‹è¯•é…ç½®: M={M}, K={K}, é˜ˆå€¼={threshold}, ç›®æ ‡ç¨€ç–åº¦={sparsity:.1%}")
        print(f"{'='*80}")
    
    # ç”Ÿæˆæµ‹è¯•æ•°æ®
    activation = generate_sparse_tensor(M, K, sparsity)
    
    # è®¡ç®—å®é™…ç¨€ç–åº¦
    _, _, row_offsets = thr_sparsify_to_icsr_sve(activation, threshold)
    actual_nnz = row_offsets[-1].item()
    actual_sparsity = 1 - actual_nnz / (M * K)
    
    if verbose:
        print(f"å®é™…éé›¶å…ƒç´ : {actual_nnz:,} / {M*K:,}")
        print(f"å®é™…ç¨€ç–åº¦: {actual_sparsity:.2%}\n")
    
    # é¢„çƒ­
    warmup_runs(thr_sparsify_to_icsr_sve, activation, threshold, warmup)
    warmup_runs(thr_sparsify_to_icsr_sve_baseline, activation, threshold, warmup)
    
    # SVE2 Compact ç‰ˆæœ¬æµ‹è¯•
    if verbose:
        print("æµ‹è¯• SVE2 Compact ç‰ˆæœ¬...")
    latency_sve2, std_sve2 = measure_performance(
        thr_sparsify_to_icsr_sve,
        activation,
        threshold,
        repeats
    )
    throughput_sve2 = (M * K) / (latency_sve2 * 1e9)  # Gå…ƒç´ /ç§’
    
    if verbose:
        print(f"  å»¶è¿Ÿ: {latency_sve2*1000:.4f} Â± {std_sve2*1000:.4f} ms")
        print(f"  ååé‡: {throughput_sve2:.3f} Gå…ƒç´ /ç§’\n")
    
    # SVE Baseline ç‰ˆæœ¬æµ‹è¯•
    if verbose:
        print("æµ‹è¯• SVE Baseline ç‰ˆæœ¬ï¼ˆæ—  compactï¼‰...")
    latency_baseline, std_baseline = measure_performance(
        thr_sparsify_to_icsr_sve_baseline,
        activation,
        threshold,
        repeats
    )
    throughput_baseline = (M * K) / (latency_baseline * 1e9)
    
    if verbose:
        print(f"  å»¶è¿Ÿ: {latency_baseline*1000:.4f} Â± {std_baseline*1000:.4f} ms")
        print(f"  ååé‡: {throughput_baseline:.3f} Gå…ƒç´ /ç§’\n")
    
    # è®¡ç®—åŠ é€Ÿæ¯”
    speedup = latency_baseline / latency_sve2
    improvement_pct = (speedup - 1) * 100
    
    if verbose:
        print(f"{'â”€'*80}")
        print(f"ğŸ“Š æ€§èƒ½å¯¹æ¯”ç»“æœ")
        print(f"{'â”€'*80}")
        print(f"  åŠ é€Ÿæ¯”:         {speedup:.3f}x")
        print(f"  æ€§èƒ½æå‡:       {improvement_pct:.2f}%")
        print(f"  æ—¶é—´èŠ‚çœ:       {(latency_baseline-latency_sve2)*1000:.4f} ms")
        print(f"{'â”€'*80}")
    
    return {
        'M': M,
        'K': K,
        'threshold': threshold,
        'target_sparsity': sparsity,
        'actual_sparsity': actual_sparsity,
        'actual_nnz': actual_nnz,
        'latency_sve2': latency_sve2,
        'std_sve2': std_sve2,
        'latency_baseline': latency_baseline,
        'std_baseline': std_baseline,
        'speedup': speedup,
        'improvement_pct': improvement_pct,
        'throughput_sve2': throughput_sve2,
        'throughput_baseline': throughput_baseline,
    }


def print_summary_table(results: List[Dict]):
    """æ‰“å°ç»“æœæ±‡æ€»è¡¨æ ¼"""
    print("\n" + "="*100)
    print("ğŸ“ˆ æµ‹è¯•ç»“æœæ±‡æ€»è¡¨")
    print("="*100)
    print(f"{'çŸ©é˜µå°ºå¯¸':<15} {'å®é™…ç¨€ç–åº¦':<12} {'SVE2 (ms)':<13} {'Baseline (ms)':<15} {'åŠ é€Ÿæ¯”':<12} {'æ€§èƒ½æå‡':<12}")
    print("-"*100)
    
    for r in results:
        config = f"{r['M']}Ã—{r['K']}"
        print(
            f"{config:<15} "
            f"{r['actual_sparsity']*100:>6.2f}%      "
            f"{r['latency_sve2']*1000:>9.4f}    "
            f"{r['latency_baseline']*1000:>11.4f}      "
            f"{r['speedup']:>8.3f}x    "
            f"{r['improvement_pct']:>7.2f}%"
        )
    
    print("="*100)


def print_statistics(results: List[Dict]):
    """æ‰“å°ç»Ÿè®¡åˆ†æ"""
    speedups = [r['speedup'] for r in results]
    improvements = [r['improvement_pct'] for r in results]
    
    print("\n" + "="*80)
    print("ğŸ“Š ç»Ÿè®¡åˆ†æ")
    print("="*80)
    print(f"  æµ‹è¯•åœºæ™¯æ•°é‡:       {len(results)}")
    print(f"  å¹³å‡åŠ é€Ÿæ¯”:         {np.mean(speedups):.3f}x")
    print(f"  ä¸­ä½æ•°åŠ é€Ÿæ¯”:       {np.median(speedups):.3f}x")
    print(f"  æœ€å¤§åŠ é€Ÿæ¯”:         {np.max(speedups):.3f}x")
    print(f"  æœ€å°åŠ é€Ÿæ¯”:         {np.min(speedups):.3f}x")
    print(f"  åŠ é€Ÿæ¯”æ ‡å‡†å·®:       {np.std(speedups):.3f}")
    print(f"  å¹³å‡æ€§èƒ½æå‡:       {np.mean(improvements):.2f}%")
    print("="*80)


def print_conclusion(results: List[Dict]):
    """æ‰“å°æµ‹è¯•ç»“è®º"""
    avg_speedup = np.mean([r['speedup'] for r in results])
    
    print("\n" + "="*80)
    print("ğŸ¯ æµ‹è¯•ç»“è®º")
    print("="*80)
    
    if avg_speedup >= 1.5:
        status = "âœ… æ˜¾è‘—æ€§èƒ½æå‡"
        recommendation = "å¼ºçƒˆæ¨èåœ¨ç”Ÿäº§ç¯å¢ƒä¸­ä½¿ç”¨ SVE2 ä¼˜åŒ–ç‰ˆæœ¬"
    elif avg_speedup >= 1.2:
        status = "âœ… æ˜æ˜¾æ€§èƒ½æå‡"
        recommendation = "æ¨èåœ¨æ”¯æŒ SVE2 çš„ç¡¬ä»¶ä¸Šä½¿ç”¨ä¼˜åŒ–ç‰ˆæœ¬"
    elif avg_speedup >= 1.05:
        status = "âš ï¸  å°å¹…æ€§èƒ½æå‡"
        recommendation = "æ”¶ç›Šç›¸å¯¹æœ‰é™ï¼Œå¯æ ¹æ®å®é™…åœºæ™¯é€‰æ‹©"
    else:
        status = "âš ï¸  æ”¶ç›Šä¸æ˜æ˜¾"
        recommendation = "å¯èƒ½å—åˆ°å…¶ä»–ç“¶é¢ˆé™åˆ¶ï¼Œå»ºè®®è¿›ä¸€æ­¥åˆ†æ"
    
    print(f"{status}")
    print(f"  SVE2 compact æŒ‡ä»¤å¹³å‡åŠ é€Ÿæ¯”: {avg_speedup:.3f}x")
    print(f"  å¹³å‡æ€§èƒ½æå‡: {(avg_speedup-1)*100:.2f}%")
    print(f"\nå»ºè®®: {recommendation}")
    print("="*80)


def run_quick_test():
    """å¿«é€Ÿæµ‹è¯•ï¼ˆå°‘é‡é…ç½®ï¼‰"""
    print("\n" + "ğŸš€ è¿è¡Œå¿«é€Ÿæµ‹è¯•æ¨¡å¼")
    
    test_configs = [
        # (M, K, threshold, sparsity)
        (512, 4096, 0.01, 0.5),
        (1024, 4096, 0.01, 0.7),
        (2048, 4096, 0.01, 0.9),
    ]
    
    results = []
    for M, K, threshold, sparsity in test_configs:
        result = run_single_benchmark(M, K, threshold, sparsity, warmup=3, repeats=20)
        results.append(result)
    
    print_summary_table(results)
    print_statistics(results)
    print_conclusion(results)


def run_comprehensive_test():
    """å…¨é¢æµ‹è¯•ï¼ˆå¤šç§é…ç½®ï¼‰"""
    print("\n" + "ğŸ”¬ è¿è¡Œå…¨é¢æµ‹è¯•æ¨¡å¼")
    
    test_configs = [
        # ä¸åŒçŸ©é˜µå°ºå¯¸
        (128, 2048, 0.01, 0.5),
        (128, 4096, 0.01, 0.5),
        (256, 4096, 0.01, 0.5),
        (512, 4096, 0.01, 0.5),
        (1, 4096, 0.01, 0.5),
        
        # ä¸åŒç¨€ç–åº¦
        (128, 4096, 0.01, 0.3),
        (128, 4096, 0.01, 0.5),
        (128, 4096, 0.01, 0.7),
        (128, 4096, 0.01, 0.8),
        (128, 4096, 0.01, 0.9),
        (128, 4096, 0.01, 0.95),
        
        # ä¸åŒå½¢çŠ¶
        (512, 8192, 0.01, 0.7),   # å®½çŸ©é˜µ
        (2048, 2048, 0.01, 0.7),  # æ­£æ–¹å½¢
        (4096, 1024, 0.01, 0.7),  # é«˜çŸ©é˜µ
    ]
    
    results = []
    for M, K, threshold, sparsity in test_configs:
        result = run_single_benchmark(M, K, threshold, sparsity, warmup=5, repeats=50, verbose=False)
        # æ‰“å°ç®€è¦è¿›åº¦
        print(f"âœ“ {M}Ã—{K} (ç¨€ç–åº¦ {sparsity:.0%}): åŠ é€Ÿæ¯” {result['speedup']:.3f}x")
        results.append(result)
    
    print_summary_table(results)
    print_statistics(results)
    print_conclusion(results)


def run_custom_test(M: int, K: int, threshold: float, sparsity: float, 
                    warmup: int, repeats: int):
    """è‡ªå®šä¹‰æµ‹è¯•"""
    print("\n" + "âš™ï¸  è¿è¡Œè‡ªå®šä¹‰æµ‹è¯•")
    result = run_single_benchmark(M, K, threshold, sparsity, warmup, repeats)
    print_conclusion([result])


def verify_correctness():
    """éªŒè¯æ­£ç¡®æ€§"""
    print("\n" + "="*80)
    print("ğŸ” æ­£ç¡®æ€§éªŒè¯")
    print("="*80)
    
    test_cases = [
        (128, 2048, 0.01, 0.5),
        (512, 4096, 0.01, 0.7),
        (1024, 4096, 0.01, 0.9),
    ]
    
    all_passed = True
    for M, K, threshold, sparsity in test_cases:
        activation = generate_sparse_tensor(M, K, sparsity)
        passed = verify_output_consistency(activation, threshold)
        
        status = "âœ… é€šè¿‡" if passed else "âŒ å¤±è´¥"
        print(f"  {M}Ã—{K} (ç¨€ç–åº¦ {sparsity:.0%}): {status}")
        
        if not passed:
            all_passed = False
    
    print("="*80)
    if all_passed:
        print("âœ… æ‰€æœ‰æ­£ç¡®æ€§æ£€æŸ¥é€šè¿‡ï¼\n")
    else:
        print("âŒ éƒ¨åˆ†æ­£ç¡®æ€§æ£€æŸ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç®—å­å®ç°ï¼\n")
        sys.exit(1)


def main():
    parser = argparse.ArgumentParser(
        description="ICSR SVE Baseline vs SVE2 Compact æ€§èƒ½å¯¹æ¯”æµ‹è¯•",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
æµ‹è¯•æ¨¡å¼:
  quick         - å¿«é€Ÿæµ‹è¯•ï¼ˆ3ä¸ªé…ç½®ï¼Œç”¨æ—¶çº¦1åˆ†é’Ÿï¼‰
  comprehensive - å…¨é¢æµ‹è¯•ï¼ˆ14ä¸ªé…ç½®ï¼Œç”¨æ—¶çº¦5-10åˆ†é’Ÿï¼‰
  custom        - è‡ªå®šä¹‰æµ‹è¯•ï¼ˆéœ€è¦æä¾›å‚æ•°ï¼‰
  verify        - ä»…è¿è¡Œæ­£ç¡®æ€§éªŒè¯

ç¤ºä¾‹:
  python %(prog)s --mode quick
  python %(prog)s --mode comprehensive
  python %(prog)s --mode custom -M 1024 -K 4096 --sparsity 0.7
  python %(prog)s --mode verify
        """
    )
    
    parser.add_argument(
        '--mode',
        type=str,
        default='quick',
        choices=['quick', 'comprehensive', 'custom', 'verify'],
        help='æµ‹è¯•æ¨¡å¼ï¼ˆé»˜è®¤: quickï¼‰'
    )
    
    # è‡ªå®šä¹‰æ¨¡å¼å‚æ•°
    parser.add_argument('-M', type=int, default=128, help='çŸ©é˜µè¡Œæ•°ï¼ˆä»… custom æ¨¡å¼ï¼‰')
    parser.add_argument('-K', type=int, default=4096, help='çŸ©é˜µåˆ—æ•°ï¼ˆä»… custom æ¨¡å¼ï¼‰')
    parser.add_argument('--threshold', type=float, default=0.01, help='ç¨€ç–åŒ–é˜ˆå€¼ï¼ˆä»… custom æ¨¡å¼ï¼‰')
    parser.add_argument('--sparsity', type=float, default=0.7, help='ç›®æ ‡ç¨€ç–åº¦ï¼ˆä»… custom æ¨¡å¼ï¼‰')
    parser.add_argument('--warmup', type=int, default=5, help='é¢„çƒ­æ¬¡æ•°ï¼ˆä»… custom æ¨¡å¼ï¼‰')
    parser.add_argument('--repeats', type=int, default=50, help='é‡å¤æµ‹è¯•æ¬¡æ•°ï¼ˆä»… custom æ¨¡å¼ï¼‰')
    
    parser.add_argument('--no-verify', action='store_true', help='è·³è¿‡æ­£ç¡®æ€§éªŒè¯')
    parser.add_argument('--verbose', action='store_true', help='è¯¦ç»†è¾“å‡ºï¼ˆä»… custom æ¨¡å¼ï¼‰')
    
    args = parser.parse_args()
    
    print("\n" + "="*80)
    print("ICSR SVE Baseline vs SVE2 Compact æ€§èƒ½å¯¹æ¯”æµ‹è¯•")
    print("="*80)
    print(f"æµ‹è¯•æ¨¡å¼: {args.mode}")
    print("="*80)
    
    # åŠ è½½æ‰©å±•
    print("\nåŠ è½½ C++ æ‰©å±•...")
    load_sve_sparse_gemm_extension(verbose=False)
    print("âœ… æ‰©å±•åŠ è½½æˆåŠŸ")
    
    # æ­£ç¡®æ€§éªŒè¯ï¼ˆé™¤éæŒ‡å®šè·³è¿‡ï¼‰
    if not args.no_verify and args.mode != 'verify':
        verify_correctness()
    
    # è¿è¡Œæµ‹è¯•
    if args.mode == 'quick':
        run_quick_test()
    elif args.mode == 'comprehensive':
        run_comprehensive_test()
    elif args.mode == 'custom':
        run_custom_test(
            args.M, args.K, args.threshold, args.sparsity,
            args.warmup, args.repeats
        )
    elif args.mode == 'verify':
        verify_correctness()
    
    print("\nâœ… æµ‹è¯•å®Œæˆï¼\n")


if __name__ == "__main__":
    main()
